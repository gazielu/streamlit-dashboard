{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20a3fb3a-fd2d-496d-8d7e-a069742b23b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Load txt to parquet -  ingestion phase\n",
    "\n",
    "1) Dataframe schema definition<BR> \n",
    "2) Read data from azure Row folder to landing zone<BR>\n",
    "3) Create Audit Log ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.Setup Folder and Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 List files in relevent storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore') #, category=FutureWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18b65c62-9023-4010-b598-d3ad93445b3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "This is an example of how to list things you need to use the software and how to install them.\n",
    "* Azure blob storage indigo \n",
    "  ```sh\n",
    " display(dbutils.fs.ls(srcDataDirRoot))\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc1c703b-1769-4251-8979-eccb65f80f4d",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MOUNTPOINT = \"c:\\BI\\GitHub\\LearningSparkV2-master\\SuppliesML\"\n",
    "SourcePath = os.path.join(MOUNTPOINT, \"data\")\n",
    "RawPath    = os.path.join(SourcePath,\"Raw\")\n",
    "ResearchPath = os.path.join(SourcePath,\"Research\")\n",
    "LandingPath = os.path.join(SourcePath, \"Landing\")\n",
    "LoadingPath = os.path.join(SourcePath,  \"Loading\")\n",
    "certified_pit = os.path.join(SourcePath,\"certified-pit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3e57f21-22a5-4fd1-9205-b35507b0929f",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define source and destination directories\n",
    "srcDataDirRoot = RawPath #Root dir for source data\n",
    "destDataDirRoot = LandingPath #Root dir for consumable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1db5b1e7-b1b3-460e-ab3f-cd14e2e4005a",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "### 1.2 Define Schema for the file that loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed56d839-dc8c-4677-8d9f-1093a0975a8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Required for StructField, StringType, IntegerType, etc.\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "# # Space bi inc- sensor data \n",
    "ProductionRunSchema = StructType([\n",
    "  StructField(\"Product\" , StringType() ,True),\n",
    "  StructField(\"Machine\" , StringType() ,True),\n",
    "  StructField(\"FOLDER_PATH\" , StringType() ,True),\n",
    "  StructField(\"Product_category\" , StringType() ,True),\n",
    "  StructField(\"Product_eng_name\" , StringType() ,True),\n",
    "  StructField(\"Series\" , StringType() ,True),\n",
    "  StructField(\"Product_name_win\" , StringType() ,True),\n",
    "  StructField(\"Size_Flag\" , StringType() ,True),\n",
    "  StructField(\"Parameter_Name\" , StringType() ,True),\n",
    "  StructField(\"Batch\" , StringType() ,True),\n",
    "  StructField(\"SAMPLE_ID\" , FloatType() ,True),\n",
    "  StructField(\"SAMPLE_Date\" , TimestampType() ,True),\n",
    "  StructField(\"SK_Sample_Date\" , IntegerType() ,True),\n",
    "  StructField(\"Parameter_Critical_Flag\" , StringType() ,True),\n",
    "  StructField(\"Is_Sample_Deleted_Flg\" , StringType() ,True),\n",
    "  StructField(\"SAMPLE_Mean\" , FloatType() ,True),\n",
    "  StructField(\"SAMPLE_stdev\" , FloatType() ,True),\n",
    "  StructField(\"SAMPLE_Minimum\" , FloatType() ,True),\n",
    "  StructField(\"SAMPLE_Maximum\" , FloatType() ,True),\n",
    "  StructField(\"SAMPLE_Median\" , FloatType() ,True),\n",
    "  StructField(\"Spec_target\" , FloatType() ,True),\n",
    "  StructField(\"SAMPLE_Size\" , FloatType() ,True),\n",
    "  StructField(\"LSL\" , FloatType() ,True),\n",
    "  StructField(\"USL\" , FloatType() ,True),\n",
    "  StructField(\"SL_enabled\" , StringType() ,True),\n",
    "  StructField(\"CH_ID\" , FloatType() ,True),\n",
    "  StructField(\"ETL_DATE\" , TimestampType() ,True)\n",
    "])\n",
    "\n",
    "\n",
    "# windigo production line\n",
    "New_Era_Run_Details_Schema = StructType([\n",
    "StructField(\"Run_Number\" , StringType() ,True),\n",
    "StructField(\"Blanket_Serial_Number\" , StringType() ,True),  \n",
    "StructField(\"Blanket_SEQ_NR\" , StringType() ,True),  \n",
    "StructField(\"Plant_Id\" , IntegerType() ,True),\n",
    "StructField(\"Quality_Status_Id\" , IntegerType() ,True),\n",
    "StructField(\"Body_ID\" , StringType() ,True),\n",
    "StructField(\"CSL_ID\" , StringType() ,True),\n",
    "StructField(\"BLK_Quality_Status_Name\" , StringType() ,True),\n",
    "StructField(\"BLK_Quality_Status_Flag\" , IntegerType() ,True),\n",
    "StructField(\"Blanket_Legacy_Part_Nr\" , StringType() ,True),\n",
    "StructField(\"Product_Engineering_Name\" , StringType() ,True),\n",
    "StructField(\"Source_System_Modified_DateTime\" , TimestampType() ,True)\n",
    "])\n",
    "\n",
    "\n",
    "# # Customer data - lifespan\n",
    "Blanket_lifespan_installed_base_Schema = StructType([\n",
    "#StructField(\"Fact_PIP_IMPACT_RowID\", IntegerType() ,True),\n",
    "StructField(\"Blanket_Impact_RowID\", StringType() ,True),\n",
    "StructField(\"Press_Serial_Number\", StringType() ,True),\n",
    "StructField(\"BLANKETS_ID\", StringType() ,True),\n",
    "StructField(\"Replacement_DateTime\", StringType() ,True),\n",
    "StructField(\"End_User_Code\", StringType() ,True),\n",
    "StructField(\"Domain\", StringType() ,True),\n",
    "StructField(\"ROR\", StringType() ,True),\n",
    "StructField(\"Consumable_Type\", StringType() ,True),\n",
    "StructField(\"Optimized_Lifespan\", IntegerType() ,True),\n",
    "StructField(\"Is_Last_Replacement\", StringType() ,True),\n",
    "StructField(\"Is_Lifespan_Official\", StringType() ,True),\n",
    "StructField(\"Consumable_Maturity\", StringType() ,True),\n",
    "StructField(\"DOA_Count\", IntegerType() ,True),\n",
    "StructField(\"DOP_Count\", IntegerType() ,True),\n",
    "StructField(\"RowID\", IntegerType() ,True),\n",
    "StructField(\"Changed_Date_Time\", StringType() ,True),\n",
    "StructField(\"Replacement_Monthly_Date_Id\", IntegerType() ,True),\n",
    "StructField(\"ETL_Date\", StringType() ,True),\n",
    "#StructField(\"Press_Classification\", StringType() ,True),\n",
    "#StructField(\"Lifespan_Guidelines\", DoubleType() ,True),\n",
    "StructField(\"Click_Charge\", StringType() ,True),\n",
    "StructField(\"Ownership\", StringType() ,True),\n",
    "StructField(\"Product_Number\", StringType() ,True),\n",
    "StructField(\"Description\", StringType() ,True),\n",
    "StructField(\"Product_Group\", StringType() ,True),\n",
    "StructField(\"Press_Group\", StringType() ,True),\n",
    "StructField(\"Family_type\", StringType() ,True),\n",
    "StructField(\"Series\", StringType() ,True),\n",
    "StructField(\"Press_Segment\", StringType() ,True),\n",
    "StructField(\"Current_SW_Version_ID\", StringType() ,True),\n",
    "StructField(\"Customer_Name\", StringType() ,True),\n",
    "StructField(\"Site_Region\", StringType() ,True),\n",
    "StructField(\"Site_Sub_Region\", StringType() ,True),\n",
    "StructField(\"Site_Country\", StringType() ,True),\n",
    "#StructField(\"ETL_Date\" , TimestampType() ,True)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.Create Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 check warehouse Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "import pyodbc\n",
    "warehouse_location = abspath('..\\..\\..\\supplies-warehouse')\n",
    "print(warehouse_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "#warehouse_location = abspath('..\\..\\..\\supplies-warehouse')\n",
    "warehouse_location = abspath('..\\..\\..\\hive_supplies-warehouse')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\")\\\n",
    ".config(\"spark.hadoop.hive.metastore.warehouse.dir\", warehouse_location)\\\n",
    ".config('spark.driver.extraClassPath', 'C:\\Program Files\\Microsoft JDBC Driver 10.2 for SQL Server\\sqljdbc_10.2\\enu\\mssql-jdbc-10.2.1.jre8.jar') \\\n",
    ".config('spark.executor.extraClassPath', 'C:\\Program Files\\Microsoft JDBC Driver 10.2 for SQL Server\\sqljdbc_10.2\\enu\\mssql-jdbc-10.2.1.jre8.jar') \\\n",
    ".config(\"spark.sql.catalogImplementation\",\"hive\") \\\n",
    ".enableHiveSupport() \\\n",
    ".getOrCreate() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. set data source Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Set odbc Connections > user + Password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Driver={SQL Server Native Client 11.0};Server=myServerAddress;Database=myDataBase;Uid=myUsername;Pwd=myPassword;\n",
    "\n",
    "sql_native_space = pyodbc.connect(\"Driver={SQL Server Native Client 11.0};\"\n",
    "                        \"Server=gvs72076.inc.hpicorp.net,2048;\"\n",
    "                        \"Database=faomrp;\"\n",
    "                        \"Uid=SPACE_BI_IDS;\"\n",
    "                        \"Pwd=SPC-IDS!pswd;\")\n",
    "                  \n",
    "# > windows trusted authication  - when we user password please assign it to \"no\"\n",
    "# conn_GABI.close()\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = 'gvs72076.inc.hpicorp.net,2048' \n",
    "database = 'faomrp' \n",
    "username = 'SPACE_BI_IDS' \n",
    "password = 'SPC-IDS!pswd' \n",
    "odbc_SPACE_BI = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "print(odbc_SPACE_BI,database)\n",
    "\n",
    "server = '106775-2s-ag1.corp.hpicloud.net,2048' \n",
    "database = 'GABI-I' \n",
    "username = 'GABI_STG_RW_1' \n",
    "password = 'change_pw_first_login2020$' \n",
    "odbc_SPACE_BI = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "print(odbc_SPACE_BI,database)\n",
    "\n",
    "server = 'gvs72069.inc.hpicorp.net,2048' \n",
    "database = 'GABI-P' \n",
    "username = 'GABI_STG_RW_1' \n",
    "password = 'fT7_E*4U6k_m' \n",
    "odbc_GABI_P = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "print(odbc_SPACE_BI,database)\n",
    "\n",
    "\n",
    "server = 'WIN-OA97GH5SPHI.CORP.HPICLOUD.NET,1433' # 172.20.206.100\n",
    "database = 'Hubble_DWH_ITG' \n",
    "username = 'Tableau_Itg' \n",
    "password = 'Tab_itg4' \n",
    "odbc_Hubble_ITG = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "print(odbc_Hubble_ITG,database)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Set odbc Connections > user authntication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyodbc.Connection object at 0x0000023C6ADE9FE0>\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "Driver=\"{ODBC Driver 17 for SQL Server}\"\n",
    "server=\"172.20.206.100,1433\"\n",
    "database=\"HUBBLE_DWH_ITG\"\n",
    "trusted_connection=\"yes\"\n",
    "odbc_Hubble_ITG = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';Database='+database+';trusted_connection='+trusted_connection)\n",
    "print(odbc_Hubble_ITG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Driver={SQL Server Native Client 11.0};Server=myServerAddress;Database=myDataBase;Uid=myUsername;Pwd=myPassword;\n",
    "import pyodbc\n",
    "sql_native_Hubble_ITG = pyodbc.connect(\"Driver={SQL Server Native Client 11.0};\"\n",
    "                        \"Server=172.20.206.100,1433;\"\n",
    "                        \"Database=HUBBLE_DWH_ITG;\"\n",
    "                        \"Uid=Tableau_Itg;\"\n",
    "                        \"Pwd=Tab_itg4;\")\n",
    "                  \n",
    "# > windows trusted authication  - when we user password please assign it to \"no\"\n",
    "# conn_GABI.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Set odbc Connections > user + passward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqlsUrl = 'jdbc:sqlserver://172.20.206.100:1433;encrypt=true;databaseName=HUBBLE_DWH_ITG;integratedSecurity=true;trustServerCertificate=true;'\n",
    "\n",
    ">> there is no integrated security "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 set jdbc Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqlsUrl = 'jdbc:sqlserver://gvs72069.inc.hpicorp.net:2048;encrypt=true;database=GABI-P;user=GABI_STG_RW_1;password=fT7_E*4U6k_m;trustServerCertificate=true;'\n",
    "JdbcsqlsUrl_space = 'jdbc:sqlserver://gvs72076.inc.hpicorp.net:2048;encrypt=true;database=faomrp;user=SPACE_BI_IDS;password=SPC-IDS!pswd;trustServerCertificate=true;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JdbcsqlsUrl_NewEra = 'jdbc:sqlserver://gvs72069.inc.hpicorp.net:2048;encrypt=true;database=GABI-P;user=GABI_STG_RW_1;password=fT7_E*4U6k_m;trustServerCertificate=true;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JdbcsqlsUrl_blanketslifespan = 'jdbc:sqlserver://WIN-OA97GH5SPHI.CORP.HPICLOUD.NET:1433;encrypt=true;database=HUBBLE_DWH_ITG;user=Tableau_Itg;password=Tab_itg4;trustServerCertificate=true;'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. List of queires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Load with JDBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5.1.1 Load from New Era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta,date\n",
    "# Build Dynamic Query per yearmonth filter\n",
    "def Loaddata(YearMonth,StartYear,querypath):\n",
    "    \n",
    "    #YearMonth =\"201903\"\n",
    "    Year=YearMonth[0:4]\n",
    "    with open(querypath) as f: query_file = f.read()\n",
    "    sql_string = query_file.replace(\"{YearMonth}\",str(YearMonth)).replace(\"{StartYear}\",str(StartYear))\n",
    "    dynamic_sql = f\"\"\"{sql_string}\"\"\"\n",
    "    return dynamic_sql\n",
    "\n",
    "\n",
    "# Read Query from Source >  Return DATAFRAME per yearmonth filter\n",
    "def sprak_read_df(Query,schema):\n",
    "    \n",
    "    startTime = time.time()\n",
    "    spark_read_df = spark.read.format('jdbc') \\\n",
    "    .option('url',JdbcsqlsUrl_NewEra) \\\n",
    "    .option('driver', 'com.microsoft.sqlserver.jdbc.SQLServerDriver')\\\n",
    "    .option('query', Query) \\\n",
    "    .schema(schema) \\\n",
    "    .load()\n",
    "    executionTime = (time.time() - startTime)\n",
    "    #print('Execution time in seconds: ' + str(round(executionTime,2)))\n",
    "    return spark_read_df\n",
    "\n",
    "\n",
    "def WriteToParqurtFile(Dataframe,Zone,Table_Folder,Year,YearMonth):\n",
    "    Dataframe.coalesce(8).write.parquet(LandingPath + Table_Folder +str(Year)+\"/\"+str(YearMonth),mode=\"overwrite\") \n",
    "    executionTime = (time.time() - startTime)\n",
    "\n",
    "min_query_date= '2020-11-01'  \n",
    "StartYear = \"2018\"\n",
    "Zone = LandingPath\n",
    "Table_Folder = \"/New_Era/New_era_details/\"\n",
    "Table = \"NewEra\"\n",
    "QueryPath = \"IngestQueries/NewEra.sql\"\n",
    "startPipelineTime =  time.time()\n",
    "\n",
    "#for beg in pd.date_range(min_query_date,date.today().strftime(\"%Y-%m-01\"), freq='MS'): # date.today() date.today().strftime(\"%Y-%m-01\")\n",
    "for beg in pd.date_range('2020-10-01','2022-11-01', freq='MS'): # date.today()\n",
    "    startTime = time.time()\n",
    "    Yearmonth = beg.strftime(\"%Y%m\")\n",
    "    Year=Yearmonth[0:4]                                     \n",
    "    dynamic_sql = Loaddata(Yearmonth,StartYear,QueryPath)   \n",
    "    \n",
    "    Dataframe = sprak_read_df(dynamic_sql,New_Era_Run_Details_Schema)\n",
    "    #WriteToParqurtFile(Dataframe,Zone,Table_Folder,Year,Yearmonth)\n",
    "    \n",
    "    executionTime = (time.time() - startTime)\n",
    "    print(f'Execution time in seconds for: Table '+Table+' > YearMonth: '+ Yearmonth +\"  is: \"+ str(round(executionTime,2))+ \"   numberofrow:  \" + str(Dataframe.count()))\n",
    "executionTime = (time.time() - startPipelineTime)\n",
    "print(f'total Execution time in seconds for: Table '+Table+' > all yearmonth is: '+ str(round(executionTime,2)))   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2.1 read & write & Test Count / Show() all NEW ERA yearmonth file to one parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_era_details_dfs = spark.read.format(\"parquet\").load(LandingPath + \"/New_Era/New_era_details/*/*\") # try to read empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_era_details_dfs.createOrReplaceTempView(\"ld_New_Eras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_new_era_Yield_df = spark.sql(\n",
    "\"select Run_Number,CAST (Sum(BLK_Quality_Status_Flag)/Count(Blanket_SEQ_NR) AS Decimal(5,2)) AS Yield  from ld_New_Eras Group by Run_Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_New_Era =New_era_details_dfs.join(ld_new_era_Yield_df,New_era_details_dfs.Run_Number ==  ld_new_era_Yield_df.Run_Number,\"left\").drop(ld_new_era_Yield_df.Run_Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_New_Era_new = spark.sql(\"select *, Year(Source_System_Modified_DateTime)*100+Month(Source_System_Modified_DateTime) As YearMonth from ld_New_Era\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "startTime = time.time()\n",
    "New_era_details_dfs.coalesce(1).write.parquet(LandingPath + \"/New_Era/New_era_details_Full/New_era_details_Full.parquet\",mode=\"overwrite\") #e\")#.saveAsTable(\"ld_blanket_lifespan_run_extended\")# = spark.read.format(\"parquet\").load(SourcePath + '/test')\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(round(executionTime,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5.1.2 Load from SpaceBI > production run jdbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta,date\n",
    "\n",
    "# Build Dynamic Query per yearmonth filter\n",
    "def Loaddata(YearMonth,StartYear,querypath):\n",
    "    \n",
    "    #YearMonth =\"201903\"\n",
    "    Year=YearMonth[0:4]\n",
    "    with open(querypath) as f: query_file = f.read()\n",
    "    sql_string = query_file.replace(\"{YearMonth}\",str(YearMonth)).replace(\"{StartYear}\",str(StartYear)).replace(\"{FolderPath}\",str(\"'\"+FolderPath+\"'\")).replace(\"{ProductName}\",str( \"'\"+ProductName+\"'\"))\n",
    "    dynamic_sql = f\"\"\"{sql_string}\"\"\"\n",
    "    #print(dynamic_sql)\n",
    "    #print(YearMonth)\n",
    "    return dynamic_sql\n",
    "\n",
    "\n",
    "# Read Query from Source >  Return DATAFRAME per yearmonth filter\n",
    "def sprak_read_df(Query,schema,conn):\n",
    "    \n",
    "    startTime = time.time()\n",
    "    spark_read_df = spark.read.format('jdbc') \\\n",
    "    .option('url',conn) \\\n",
    "    .option('driver', 'com.microsoft.sqlserver.jdbc.SQLServerDriver')\\\n",
    "    .option('query', Query) \\\n",
    "    .schema(schema) \\\n",
    "    .load()\n",
    "    executionTime = (time.time() - startTime)\n",
    "    #print('Execution time in seconds: ' + str(round(executionTime,2)))\n",
    "    return spark_read_df\n",
    "\n",
    "def pandas_read_df(dynamic_sql,sql_native_space):\n",
    "    fmt='%Y%m%d %H:%M:%S'             \n",
    "    pandas_read_pdf = pd.read_sql_query(sql = dynamic_sql,\\\n",
    "                                        con = sql_native_space,\\\n",
    "                                        parse_dates={'SAMPLE_Date':fmt,'ETL_DATE':fmt}\\\n",
    "                                       )\n",
    "                                        #parse_dates={'Date_of_creation':fmt, \\ \n",
    "                                        #'Date_of_termination':fmt})  \n",
    "    return pandas_read_pdf\n",
    "\n",
    "def WriteToParqurtFile(Dataframe,Zone,Table_Folder,Year,Yearmonth):\n",
    "    spark_read_df=spark.createDataFrame(Dataframe,ProductionRunSchema) #ProductionRunSchema\n",
    "    spark_read_df.coalesce(1).write.parquet(LandingPath + Table_Folder +str(Year)+\"/\"+str(Yearmonth),mode=\"overwrite\") \n",
    "    executionTime = (time.time() - startTime)\n",
    "#Blanket_lifespan_installed_base_Schema\n",
    "\n",
    "# def PandasWriteToParqurtFile(Dataframe,Zone,Table_Folder,Year,YearMonth):\n",
    "#     Dataframe.to_parquet(LandingPath + Table_Folder +str(Year)+\"/\"+str(YearMonth),mode=\"overwrite\") \n",
    "#     executionTime = (time.time() - startTime)\n",
    "# Blanket_lifespan_installed_base_Schema\n",
    "\n",
    "\n",
    "\n",
    "ProductList = {}\n",
    "ProductList['Gemini3'] = {'StartYearMonth': '2020-10-01', 'FullPath': 'New Era+AFL+WSM-> Gemini 3','FolderOutput':'Gemini', 'ProductName': 'GEMINI3'}\n",
    "ProductList['RotemSP'] = {'StartYearMonth': '2020-11-01', 'FullPath': 'New Era+AFL+WSM-> NewEra-SpeedUp-> Rotem SP','FolderOutput':'RotemSP', 'ProductName': 'ROTEM_SP'}\n",
    "ProductList['IrisPlusSP'] = {'StartYearMonth': '2020-10-01', 'FullPath': 'New Era+AFL+WSM-> NewEra-SpeedUp-> Iris Plus SP','FolderOutput':'IrisPlusSP', 'ProductName': 'IRIS PLUS_SP'}\n",
    "ProductList['Timna2'] = {'StartYearMonth': '2020-10-01', 'FullPath': 'New Era+AFL+WSM-> Timna 2','FolderOutput':'Timna2', 'ProductName': 'TIMNA 2'}\n",
    "\n",
    "\n",
    "### Production run queries variables\n",
    "## **************************************\n",
    "# find Min Date To load Data\n",
    "querypath = \"IngestQueries/ProductionLine.sql\"\n",
    "min_date = '2019-01-01'\n",
    "StartYear = \"2018\"\n",
    "FolderPath = ProductList['IrisPlusSP']['FullPath']\n",
    "ProductName = ProductList['IrisPlusSP']['ProductName']\n",
    "with open(querypath) as f: query_file = f.read()\n",
    "sql_string = query_file.replace(\"{StartYear}\",str(StartYear)).replace(\"{FolderPath}\",str(\"'\"+FolderPath+\"'\")).replace(\"{ProductName}\",str( \"'\"+ProductName+\"'\"))\n",
    "dynamic_sql = f\"\"\"{sql_string}\"\"\"\n",
    "#print(str(\"'\"+FolderPath+\"'\"))\n",
    "min_query_date_pdf = pd.read_sql_query(sql = dynamic_sql,  con = sql_native_space)  \n",
    "start_date = min_query_date_pdf['MinYearMonth'][0].strftime(\"%Y-%m-01\")\n",
    "min_query_date = str(max(min_date,start_date))\n",
    "## **************************************\n",
    "\n",
    "min_query_date= '2022-12-01'\n",
    "    \n",
    "Zone = LandingPath\n",
    "product = \"IrisPlusSP\"   # Iris 6234  '2022-11-01'  # rptem Count before inner 2460\n",
    "Table_Folder = \"/ProductionRun/ProductionRun_hist/\"+product+\"/\"\n",
    "#Table_Folder = \"/New_Era/New_era_details/\"\n",
    "Table = \"ProductionRun\"\n",
    "QueryPath = \"IngestQueries/ProductionRun.sql\"\n",
    "startPipelineTime =  time.time()\n",
    "for beg in pd.date_range(min_query_date,date.today().strftime(\"%Y-%m-01\"), freq='MS'): # date.today() date.today().strftime(\"%Y-%m-01\")\n",
    "    startTime = time.time()\n",
    "    Yearmonth = beg.strftime(\"%Y%m\")\n",
    "    Year=Yearmonth[0:4]                                     \n",
    "    dynamic_sql = Loaddata(Yearmonth,StartYear,QueryPath)     \n",
    "    print('finish dynmic',Yearmonth)\n",
    "    Dataframe = pandas_read_df(dynamic_sql,sql_native_space)\n",
    "    print('finish READ QUERY')\n",
    "    WriteToParqurtFile(Dataframe,Zone,Table_Folder,Year,Yearmonth)\n",
    "    \n",
    "    executionTime = (time.time() - startTime)\n",
    "    print(f'Execution time in seconds for: Table '+Table+' > YearMonth: '+ Yearmonth +\"  is: \"+ str(round(executionTime,2))+ \"   numberofrow:  \" + str(len(Dataframe.index)))\n",
    "\n",
    "executionTime = (time.time() - startPipelineTime)\n",
    "print(f'{datetime.now()} +   total Execution time in seconds for: Table '+Table+' > all yearmonth is: '+ str(round(executionTime,2)))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.1.2.1 read & write & Test Count / Show() all space BI yearmonth file to one parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters_run_level_dfs = spark.read.format(\"parquet\").load(LandingPath + \"/ProductionRun/ProductionRun_hist/*/*/*\")\n",
    "Parameters_run_level_dfs = spark.read.format(\"parquet\").load(LandingPath + \"/ProductionRun/ProductionRun_hist/*/*/*/*\") # try to read empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parameters_run_level_dfs.createOrReplaceTempView(\"ld_Parameters_run_level_dfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_Parameters_run_level_dfs_new = spark.sql(\"select *, Year(SAMPLE_Date)*100+Month(SAMPLE_Date) As YearMonth from ld_Parameters_run_level_dfs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_Parameters_run_level_dfs_new.createOrReplaceTempView(\"ld_Parameters_run_level_dfs_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time in seconds: 6.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "494670"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "ld_Parameters_run_level_dfs_new.coalesce(1).write.parquet(LandingPath + \"/ProductionRun/ProductionRun_Full/ProductionRun_Full_YM.parquet\",mode=\"overwrite\") #e\")#.saveAsTable(\"ld_blanket_lifespan_run_extended\")# = spark.read.format(\"parquet\").load(SourcePath + '/test')\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(round(executionTime,2)))\n",
    "Parameters_run_level_dfs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2.1 TEST Load from SpaceBI > production run jdbc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load With pyodbc - Blankets Lifespan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_read_sql_query\n",
      "after_convert_in_pandas\n",
      "Execution time in seconds for: Table Blanketslifespan > YearMonth: 202012  is: 210.57   numberofrow:  69502\n",
      "after_read_sql_query\n",
      "after_convert_in_pandas\n",
      "Execution time in seconds for: Table Blanketslifespan > YearMonth: 202101  is: 77.29   numberofrow:  59427\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta,date\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, LongType\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Build Dynamic Query per yearmonth filter\n",
    "def Loaddata(YearMonth,StartYear,querypath):\n",
    "    \n",
    "    #YearMonth =\"201903\"\n",
    "    Year=YearMonth[0:4]\n",
    "    with open(querypath) as f: query_file = f.read()\n",
    "    sql_string = query_file.replace(\"{YearMonth}\",str(YearMonth)).replace(\"{StartYear}\",str(StartYear))\n",
    "    dynamic_sql = f\"\"\"{sql_string}\"\"\"\n",
    "    #print(dynamic_sql)\n",
    "    #print(YearMonth)\n",
    "    return dynamic_sql\n",
    "\n",
    "\n",
    "convert_dict = {#'Blanket_Impact_RowID': str,\n",
    "                'Optimized_Lifespan': int,\n",
    "                #'Replacement_DateTime': str\n",
    "                }\n",
    " \n",
    "\n",
    "\n",
    "# Read Query from Source >  Return DATAFRAME per yearmonth filter\n",
    "def pandas_read_df(query,conn):   \n",
    "    startTime = time.time()\n",
    "    Pareto_pdf = pd.read_sql_query(sql = query, con = conn)     \n",
    "    print(\"after_read_sql_query\")\n",
    "    \n",
    "    Pareto_pdf = Pareto_pdf.astype(convert_dict)\n",
    "    #Pareto_pdf = Pareto_pdf.tz_localize(tz=\"Asia/Jerusalem\", nonexistent=pd.Timedelta('1H'))\n",
    "    Pareto_pdf['ETL_Date'] = pd.to_datetime(Pareto_pdf['ETL_Date']) \\\n",
    ".dt.tz_localize('Asia/Jerusalem',nonexistent=pd.Timedelta('1H'))\n",
    "    print(\"after_convert_in_pandas\")\n",
    "    return Pareto_pdf\n",
    "\n",
    "\n",
    "def WriteToParqurtFile(Dataframe,Zone,Table_Folder,Year,YearMonth):\n",
    "    #Dataframe = Dateframe['Blanket_Impact_RowID'].astype('int')\n",
    "    spark_read_df_temp =spark.createDataFrame(Dataframe,Blanket_lifespan_installed_base_Schema)\n",
    "    spark_read_df_temp.coalesce(8).write.parquet(LandingPath + Table_Folder +str(Year)+\"/\"+str(YearMonth),mode=\"overwrite\") \n",
    "    executionTime = (time.time() - startTime)\n",
    "\n",
    "    \n",
    "Zone = LandingPath\n",
    "#Table_Folder = \"/Blanket_lifespan/Blanket_lifespan_Hist_New/\"\n",
    "Table_Folder = \"/Blanket_lifespan/Blanket_lifespan_Hist_Test_pandas_n/\"\n",
    "Table = \"Blanketslifespan\"\n",
    "QueryPath = \"IngestQueries/Blanketslifespan.sql\"\n",
    "#Conn = odbc_Hubble_ITG\n",
    "StartYear = \"2019\"\n",
    "startPipelineTime =  time.time()\n",
    "for beg in pd.date_range('2020-12-01','2022-12-01', freq='MS'): # date.today()\n",
    "    startTime = time.time()\n",
    "    YearMonth = beg.strftime(\"%Y%m\")\n",
    "    Year=YearMonth[0:4]                                     \n",
    "    dynamic_sql = Loaddata(YearMonth,StartYear,QueryPath)  \n",
    "    #print(dynamic_sql)\n",
    "    Dataframe = pandas_read_df(dynamic_sql,odbc_Hubble_ITG  ) #sql_native_Hubble_ITG\n",
    "    Dataframe.to_parquet(LandingPath + Table_Folder +\"/\"+str(YearMonth)+\".parquet\")\n",
    "    #WriteToParqurtFile(Dataframe,Zone,Table_Folder,Year,YearMonth)\n",
    "    executionTime = (time.time() - startTime)\n",
    "    print(f'Execution time in seconds for: Table '+Table+' > YearMonth: '+ YearMonth +\"  is: \"+ str(round(executionTime,2))+ \"   numberofrow:  \" + str(len(Dataframe.index)))\n",
    "    \n",
    "executionTime = (time.time() - startPipelineTime)\n",
    "print(f'{datetime.now()} total Execution time in seconds for: Table '+Table+' > all yearmonth is: '+ str(round(executionTime,2))) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "Blanketslifespan = spark.read.format(\"parquet\").load(LandingPath + Table_Folder +\"/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "Blanketslifespan.coalesce(1).write.parquet(LandingPath + \"/Blanket_lifespan/Blanket_lifespan_hist_PM.parquet\",mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1727554795370864,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "1-LoadTxt2Parquet Test ETL",
   "notebookOrigID": 936863364878517,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
